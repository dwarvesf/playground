---
tags: engineering/ai, llm, foundation-model, fine-tuning
author: Pham Duc Thanh
github_id: zlatanpham
date: 2023-05-18
icy: 10
---

Foundation models are the latest advancement in the AI realm, proposed by Stanford researchers. Unlike conventional AI systems, they aren't limited to specific tasks, making them a game-changer for a variety of applications.

![](assets/foundation-model.webp)

To simplify, think of these models as a utility that can be adapted for different tasks. They learn from a wealth of unstructured data in an unsupervised way, similar to a child learning a language by listening to conversations. Language models, for example, are fed countless sentences and learn to predict the following word from previous ones. This generative nature categorizes them under 'generative AI.'

Though initially aimed at generative tasks, these models can tackle traditional Natural Language Processing (NLP) tasks such as text classification or entity recognition. This versatility comes from a process known as 'tuning,' where the model parameters are tweaked for a specific task using some labeled data. However, they can also perform well in low-labeled data scenarios via 'prompting'.

These models boast significant advantages, including high performance and productivity gains. Their learning from massive data sets makes them outshine models trained on fewer data points. And with 'prompting' or 'tuning,' creating a task-specific model requires much less labeled data than starting from zero.

Nevertheless, challenges exist. Training these models can be cost-intensive, which may deter smaller companies. Their operational costs can also be high, especially for large models that need multiple GPUs. Trust issues arise too, as these models might have been trained on untrustworthy internet data, possibly containing bias, hate speech, or other problematic content.

It's noteworthy that the use of foundation models isn't confined to language tasks. They are applicable across domains, whether it's image generation from text as seen with DALL-E 2, assisting in code writing like Copilot, discovering molecules in chemistry, or leveraging geospatial data in climate research.

## References
- https://research.ibm.com/blog/what-are-foundation-models
- [What are Generative AI models](https://www.youtube.com/watch?v=hfIUstzHs9A)

---
<!-- cta -->

### Contributing
At Dwarves, we encourage our people to read, write, share what we learn with others, and [[CONTRIBUTING|contributing to the Brainery]] is an important part of our learning culture. For visitors, you are welcome to read them, contribute to them, and suggest additions. We maintain a monthly pool of $1500 to reward contributors who support our journey of lifelong growth in knowledge and network.

### Love what we are doing?
- Check out our [products](https://superbits.co)
- Hire us to [build your software](https://d.foundation)
- Join us, [we are also hiring](https://github.com/dwarvesf/WeAreHiring)
- Visit our [Discord Learning Site](https://discord.gg/dzNBpNTVEZ)
- Visit our [GitHub](https://github.com/dwarvesf)