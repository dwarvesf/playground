---
tags: 
- ai
- llm
- foundation-model
- fine-tuning
authors: 
- thanh
title: 'Foundation Models: The Latest Advancement in AI'
description: An overview of foundation models, their versatility in generative AI tasks, and their impact across various domains like NLP, image generation, and scientific research. The article highlights the benefits, challenges, and future potential of these models.
github_id: zlatanpham
date: 2023-05-18
---

Foundation models are the latest advancement in the AI realm, proposed by Stanford researchers. Unlike conventional AI systems, they aren't limited to specific tasks, making them a game-changer for a variety of applications.

![](assets/foundation-model.webp)

To simplify, think of these models as a utility that can be adapted for different tasks. They learn from a wealth of unstructured data in an unsupervised way, similar to a child learning a language by listening to conversations. Language models, for example, are fed countless sentences and learn to predict the following word from previous ones. This generative nature categorizes them under 'generative AI.'

Though initially aimed at generative tasks, these models can tackle traditional Natural Language Processing (NLP) tasks such as text classification or entity recognition. This versatility comes from a process known as 'tuning,' where the model parameters are tweaked for a specific task using some labeled data. However, they can also perform well in low-labeled data scenarios via 'prompting'.

These models boast significant advantages, including high performance and productivity gains. Their learning from massive data sets makes them outshine models trained on fewer data points. And with 'prompting' or 'tuning,' creating a task-specific model requires much less labeled data than starting from zero.

Nevertheless, challenges exist. Training these models can be cost-intensive, which may deter smaller companies. Their operational costs can also be high, especially for large models that need multiple GPUs. Trust issues arise too, as these models might have been trained on untrustworthy internet data, possibly containing bias, hate speech, or other problematic content.

It's noteworthy that the use of foundation models isn't confined to language tasks. They are applicable across domains, whether it's image generation from text as seen with DALL-E 2, assisting in code writing like Copilot, discovering molecules in chemistry, or leveraging geospatial data in climate research.

## References

- https://research.ibm.com/blog/what-are-foundation-models
- [What are Generative AI models](https://www.youtube.com/watch?v=hfIUstzHs9A)
